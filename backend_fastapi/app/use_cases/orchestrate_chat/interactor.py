import json
import re
import zoneinfo
from typing import Any, Dict, List, Optional, Tuple
from .models import OrchestrateChatInput, OrchestrateChatOutput, ToolCallRecord
from app.use_cases.ports.llm_port import LLMPort
from app.use_cases.ports.mcp_port import MCPPort
from datetime import datetime

ALIAS_BY_TOOL = {
    "event_list": {
        "start_date": "start_from",
        "end_date": "end_to",
        "from": "start_from",
        "to": "end_to",
        "time_min": "start_from",
        "time_max": "end_to",
        # si alguna vez el LLM manda "start" "end" queriendo rango:
        # "start": "start_from",
        # "end": "end_to",
    },
    "event_find": {
        "start_date": "start_from",
        "end_date": "end_to",
        "from": "start_from",
        "to": "end_to",
        "time_min": "start_from",
        "time_max": "end_to",
        "summary": "title",
        "name": "title",
    },
    "event_update": {
        "selector_start_date": "selector_start_from",
        "selector_end_date": "selector_end_to",
        "from": "selector_start_from",
        "to": "selector_end_to",
        "summary": "patch_title",
        "id": "selector_event_id",
        "eventId": "selector_event_id",
    },
    "event_delete_by_title": {
        "start_date": "start_from",
        "end_date": "end_to",
        "from": "start_from",
        "to": "end_to",
        "desc": "description_contains",
        "description": "description_contains",
        "summary": "title",
        "name": "title",
    },
    "event_get": {
        "id": "event_id",
        "eventId": "event_id",
    },
    "event_delete": {
        "id": "event_id",
        "eventId": "event_id",
    },
    "event_create": {
        "summary": "title",
        "name": "title",
    },
}

def _apply_aliases(args: dict, alias_map: dict) -> dict:
    if not isinstance(args, dict):
        return args
    return {alias_map.get(k, k): v for k, v in args.items()}

def _unwrap_arguments(d: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(d, dict):
        return {}
    args = d.get("arguments")
    return args if isinstance(args, dict) else d

def _strip_think(text: Optional[str]) -> str:
    if not text:
        return ""
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)

def _is_ok_envelope(res: Any) -> Tuple[bool, Optional[str], Optional[Dict[str, Any]]]:
    """
    Devuelve (ok, say, data) si es envelope {'ok': True, 'say': '...', 'data': {...}}.
    """
    if isinstance(res, dict) and res.get("ok") is True:
        return True, res.get("say"), res.get("data")
    return False, None, None

def _is_known_error(res: Any) -> Tuple[bool, str, str, Optional[Dict[str, Any]]]:
    if isinstance(res, dict) and isinstance(res.get("error"), dict):
        err = res["error"]
        return True, str(err.get("code", "UNKNOWN")), str(err.get("message", "")), err.get("extra")
    if isinstance(res, dict) and res.get("isError"):
        return True, "RUNTIME_ERROR", str(res), None
    return False, "", "", None

def _fmt_dt(dt: str | None) -> str:
    if not dt: return ""
    return dt

def _compact_event_line(ev: dict) -> str:
    t = ev.get("title") or "(sin título)"
    s = _fmt_dt(str(ev.get("start") or ""))
    e = _fmt_dt(str(ev.get("end") or ""))
    i = ev.get("id") or ""
    return f"{t} — {s}–{e} (id: {i})"

def _render_numbered(items: list[dict], limit: int = 10) -> str:
    shown = items[:limit]
    lines = [f"{idx+1}) {_compact_event_line(ev)}" for idx, ev in enumerate(shown)]
    if len(items) > limit:
        lines.append(f"... y {len(items)-limit} más")
    return "\n".join(lines)

class OrchestrateChatInteractor:
    def __init__(self, llm: LLMPort, mcp: MCPPort):
        self.llm = llm
        self.mcp = mcp

    async def execute(self, inp: OrchestrateChatInput) -> OrchestrateChatOutput:
        tools = await self.mcp.list_tools_openai_schema()

        TZ = zoneinfo.ZoneInfo("America/Santiago")

        SYSTEM_TEXT = f"""
        Tus respuestas siempre deben ser en español.

        Zona horaria: America/Santiago.

        Ahora: {{NOW_ISO}} (siempre úsalo como referencia temporal).

        Formato de fechas: ISO8601 con zona (ej. 2025-08-26T15:00:00-04:00).
        """.replace("{NOW_ISO}", datetime.now(TZ).isoformat())

        first = await self.llm.chat(
            messages=[
                {"role": "system", "content": SYSTEM_TEXT},
                {"role": "user", "content": inp.message},
            ],
            tools=tools
        )

        if not first.tool_calls:
            return OrchestrateChatOutput(reply=_strip_think(first.content) or "OK")

        used: List[ToolCallRecord] = []
        messages: List[dict] = [
            {"role": "system", "content": SYSTEM_TEXT},
            {"role": "user", "content": inp.message},
        ]
        if first.raw_provider_message:
            messages.append(first.raw_provider_message)

        all_ok = True
        collected_say: List[str] = []
        collected_data: List[Dict[str, Any]] = []
        list_blocks: list[str] = []
        ambig_blocks: list[str] = []  
        first_error: Optional[Dict[str, Any]] = None

        for tc in first.tool_calls:
            raw_args = _unwrap_arguments(tc.args or {})
            normalized_args = _apply_aliases(raw_args, ALIAS_BY_TOOL.get(tc.name, {}))
            result = await self.mcp.call_tool(tc.name, normalized_args)

            if isinstance(result, dict) and result.get("ok") is True:
                data = result.get("data") or {}
                items = data.get("items") or data.get("events") or None
                if isinstance(items, list) and items:
                    list_blocks.append(_render_numbered(items))

            is_err, code, msg, extra = _is_known_error(result)
            if is_err and code == "AMBIGUOUS_MATCH":
                cands = (extra or {}).get("candidates") or []
                if isinstance(cands, list) and cands:
                    ambig_blocks.append(_render_numbered(cands))

            used.append(ToolCallRecord(name=tc.name, args=normalized_args, result=result))

            ok, say, data = _is_ok_envelope(result)
            if ok:
                if say:
                    collected_say.append(say)
                if data:
                    collected_data.append({"tool": tc.name, **data})
            else:
                all_ok = False
                is_err, code, msg, extra = _is_known_error(result)
                if is_err and first_error is None:
                    first_error = {"code": code, "message": msg, "extra": extra}

            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tc.name,
                "content": json.dumps({
                    "_provenance": "TOOL_RESULT",
                    "data": result
                }, ensure_ascii=False)
            })

        if all_ok and collected_say:
            return OrchestrateChatOutput(reply=" ".join(collected_say), tool_calls=used)
        
        if list_blocks:
            reply = "Encontré estos eventos:\n" + "\n".join(list_blocks)
            reply += "\n\n¿Quieres que abra uno por id o que lo actualice/elimine? Dime el id o el número."
            return OrchestrateChatOutput(reply=reply, tool_calls=used)

        if ambig_blocks:
            reply = "Hay múltiples coincidencias. Elige una opción:\n" + "\n".join(ambig_blocks)
            reply += "\n\nResponde con el número o el id del evento."
            return OrchestrateChatOutput(reply=reply, tool_calls=used)

        if first_error is not None:
            code = first_error["code"]
            if code == "AMBIGUOUS_MATCH":
                return OrchestrateChatOutput(
                    reply="Encontré varias coincidencias. ¿Cuál quieres? (puedes elegir por título y fecha)",
                    tool_calls=used,
                )
            if code == "EVENT_NOT_FOUND":
                return OrchestrateChatOutput(
                    reply="No encontré ese evento en el rango. ¿Quieres ampliar el rango o ajustar el título?",
                    tool_calls=used,
                )
            return OrchestrateChatOutput(
                reply=f"Ocurrió un error al procesar tu solicitud ({code}). ¿Probamos de nuevo con más detalles?",
                tool_calls=used,
            )

        final = await self.llm.chat(messages=messages, tools=tools)
        return OrchestrateChatOutput(reply=_strip_think(final.content) or "OK", tool_calls=used)
